---
layout: post
title: "[DRAFT] Lessons from the American Express Default Prediction Kaggle competition"
---

TODO(Andrea): editing and organization

# TIL (from my notebooks)

-   `01-EDA`
    -   `natsort` to sort mixed alphabetical-numerical items like:
        `A_1, A_2, A_3, B_1, B_2, B_3`.
    -   The parquet format is fast.
    -   Gained experience with pandas' MultiIndex (`customer_ID`, `S_2`).
-   `02-xgb`
    -   Xgboost's on GPU with `tree_method=gpu_hist` is much faster than the CPU
        version.
    -   `cudf` is super-fast (3s for feature engineering on 5M rows).
        It's not 100% equal to pandas, there are extra methods such as
        `hex_to_int`.
    -   Python's memory usage can be managed by sprinkle `del` and
        `gc.collect()` liberally.
    -   original df -> feature df(s) -> concat is a useful pattern for feature
        engineering.
    -   Groupby aggregations can be used summarize many small timeseries of
        different lenghts.
    -   Learned how to do early stopping with a custom metric and xgboost
-   `03-xgb-new`
    -   "After pay" features, first experience into combining features with an
        arithmentic operation (subtraction in this case).
    -   Experimenting with xgboost's data hyperparameters like `subsample`
        (bagging), `colsample_bytree` (feature bagging).
        And the model hyperparameters such as the learning rate `eta` and the
        `early_stopping_rounds`.
-   `04-framework`
    -   Further accelerated training with a custom metric implemented on the GPU.
    -   First time working with LGBM on both CPU and GPU.
    -   StratifiedKFold for imbalanced problems.
    -   Experiment tracking in Google Sheets
-   `05-xgb`
    -   More feature engineering and experimenting denoising via rounding to the
        second digit.
-   `06-ensembles`
    -   First time doing a mean ensemble.
-   `07-submit-rapids-xgb`
    -   Just jubmitting a public notebook to see its public score.
<!-- 08 does not exist lmao -->
-   `09-LGBM-CPU`
    -   Serious ablation studies on LGBM, learned very well the effect of the
        various hyperparameters
-   `10-NN`
    -   Increasingly getting fed-up with pandas' slowness and crazy memory usage
        as well as cudf's memory freeing issues.
        Started learning polars.
    -   First time using keras on tabular data
    -   Tried several architechtures (Dense, Resnet)
    -   Tried new activation functions mish, swish
    -   Tried several weight regularizations, dropout, batchnorm
-   `11-XGB`
    -   Serious ablation studies, this time on XGB.
        Quite proud of the spreadsheet template.
    -   More combinations of groupby features, such as `last - mean`.
-   `12-EDA-2`
    -   My pandas-fu is getting better and better.
    -   Instead of `darkgrid` also `white` + `sns.despine()` looks really good.
-   `13-DART-bruteforce`
    -   Trying to make DART train without memory issues on Kaggle's machines.
        I was not successful.
        More investigation is needed as why LGBM uses so much memory, since I
        later discovered spikes at 26GB but then training at 12GB.
-   `14-ensembling`
    -   Exponentially weighted ensemble (log-odds).
-   `15-TabNet`
    -   I just ran the notebook and used it in an ensemble.

# TIL (from post-competition discussion and top-performing solutions)

-   How to structure the pipeline for a Kaggle competition
    -   https://github.com/JEddy92/amex_default_kaggle
-   Insane number of original ideas for feature engineering
    -   https://www.kaggle.com/competitions/amex-default-prediction/discussion/348530
-   Not a surprise, but another point in favor of NNs improving GBDTs when
    ensembled
-   Hull moving average
-   More neural architectures for tabular data: TabNet, PTLS, Neural Oblivious
    Decision Ensembles
    -   Ensemble of multiple TabNets works well apparently
-   Short sequences are the reason for bad performance
    -   Length 13 sequences score 0.8214, the others score 0.6724
    -   Solution: predict the missing sequence items for customers with less
        than 13 items. We can even use the test set to train these!
    -   To do this prediction/imputation use a simple RNN (GRU + FC)
    -   https://www.kaggle.com/competitions/amex-default-prediction/discussion/347668
-   Neural networks for feature generation/imputation, not just for
    classification
    -   Train a LSTM on every feature (13 x 1 input shape) to predict the
        original target. Use the model's predictions as new features (similar
        to what P_2 is basically).
        -   https://www.kaggle.com/competitions/amex-default-prediction/discussion/347651
-   Pseudo-labeling
-   Handling missing data using attention
-   Use macroeconomic data, thinking that default rate is correlated to the
    makred. In particular, use DJI and S&P500.
-   Cursed features from Chinese quant website
    -   https://www.joinquant.com/data/dict/alpha191
-   Use Shapley values for adversarial validation
    -   https://www.kaggle.com/code/pabuoro/amex-ultra-fast-adversarial-validation-shap/notebook
-   But also a good GBDT-only ensemble (XGB, LGBM, Catboost) can reach silver
-   If you have too many features, split them in buckets, and do per-bucket
    feature importance

# Things to explore more

-   Analyze the missing values, often they are not missing at random and
    they are predictable.

-   Learning to rank

-   Catboost
-   Various NN architectures for timeseries/tabular data: FCNN, LSTM, GRU,
    Transformer
-   DART (Dropout Additive Regression Trees) works really well but it's super
    slow
-   Pyramid training schedule, interpolates between RF and GBDT
    -   https://www.kaggle.com/competitions/amex-default-prediction/discussion/338752

-   Permutation feature importance
    -   Use a stable metric though, like the Gini score part of the AMEX metric
-   Null feature importance
-   XGBoost's fast SHAP for feature importance
-   XGBoost's `num_parallel_tree`

-   RAPIDS and RAPIDS FIL

-   Rank ensembling
-   Stacking and blending, going further than just level-1 models

-   Don't only analyze the training data, also analyze the test data.
    This leads to adverarial validation and various augmentations.
-   Early stopping on validation set is overfitting your CV if you think about
    it
    Look at their distribution, stratifiy by target, group, etc.

# Top scoring solutions summary

-   73rd place
    -   Level 1: LGBM DART + GRU Encoder + Transformer
        -   Both GRU and Transformer based on cdeotte's templates.
            The improvements were:
            -   OHE categoricals
            -   NaN indicators
            -   Linear interpolation NaN filling
            -   More GRU and TransformerEncoder layers
    -   Level 2: LogisticRegression + MLP
    -   Level 3: Blending
-   46th place
    -   Level 1: XGBoost
        -   AUC metric
        -   XGBoost Pyramid
        -   Technical Analysis inspired feature engineering (e.g. Hull Moving
            Average)
        -   Small learning rate: 0.005
    -   Meta: for every row in train and test, predict the chance of missing the
        next month's payment. (lots of details I have not reported here, esp.
        w.r.t. the feature engineering)
    -   Train 4 models on full dataset wihtout early stopping
-   25th place
    -   Level 1: LGBM DART + XGBoost + CatBoost + TabNet + NODE + Transformer
        -   Logloss metric
        -   Computed all possible subtracted pairs for numerical features, with
            NaNs filled with 0. A pair is kept if the linear correlation
            (with what?) is significantly higher than both of the features'
            correlations.
            Interesting way of doing feature selection for vast amounts of
            features.
        -   Linear interpolation of last 3 months values for future prediction
        -   To train TabNet, take LGBM DART, compute OOF shap values, and use it
            as the features for NN.
            See https://www.kaggle.com/code/pavelvod/gbm-supervised-pretraining
    -   Level 2: EnsembleClassfier with forward selection process
-   17th place
    -   Level 1: LGBM DART, XGBoost, CatBoost, TabNet, NN
        -   Rolling features, date features, diff features
    -   Level 2: Ridge, Blend
    -   Pseudolabeling: generate extra data by taking predictions whose rank was
        in the 5th and 95th percentile, and use them as training samples.
-   16th place
    -   Level 1: LGBM DART, XGBoost, CatBoost, NNs, TabNet
        -   Train some models with subsets of the engineered features:
            a model using only HMA (Hull Moving Average), a model using only
            differences, a model using both.
    -   Level 2: Weighted rank ensemble optimized with Optuna
-   15th place
    -   Level 1: LGBM DART + XGBoost + CatBoost + TabNet + MLP + AutoML
        -   DART has learning rate lowered even further: 0.0075
        -   Usual features from public notebooks, kept around 1k-2k range
    -   Level 2: Log-odds ensembling with forward selection
-   14th place
    -   Level 1: LGBM DART + Transformer
        -   Transformer trained on train + LGBM OOF preds + LGBM test preds
            -   Knowledge distillation + Pseudolabeling
        -   4 layer transformer without skip connection, GRU layer after
            transformer blocks and before final classification layer
        -   LGBM Dart can be converted into GPU RAPIDS FIL for fast inference,
            super-useful for permutation importance.
    -   Level 2: Averaging
-   13th place
    -   Level 1: LGBM + XGB + CatBoost + MLP + GRU + Transformer
        -   Even more data clearning than Raddar's dataset
        -   More kfolds 5 -> 15 folds gives +13 bps improvement
        -   Longer early stopping +5 bps improvement
        -   Knowledge Distillation for all neural models
    -   Level 2: LGBM Stacking + CMA (Covariance Matrix Adaptation)
-   11th place
    -   Level 1: LGBM DART
        -   Usual features with top 50 PCA features
        -   Feature selection with split importance
        -   Meta-features
            -   Join all (unaggregated) the statements with the target
            -   Train a model to predict this
            -   Use those predictions as features in the aggregated model
-   10th place
    -   TODO(Andrea)

# Reflections

-   I need to get used to run experiments overnight
    -   I could have used more seeds
        -   e.g. a silver model uses 5 seeds for each of the two GBDTs
            and 10 seeds for the NN, so 20 in total
    -   I could have trained on the whole dataset, I'm not using early stopping
        anyway
    -   I could have done hyperparamer optimization

-   Why did feature selection not work?
    Should I have used more features?
    Should I have used it earlier?
    Base -> Blowup -> Base approach

-   Imputation of features is super important

-   For this competition the game plan is: create something original (neural)
    and blend it with the best public solution
    -   Ensembling works even with "bad" models. If I have a 0.798 and a 0.790
        model I should expect the performance not to be 0.794 if they are not
        correlated. Many winning solutions stress this.
